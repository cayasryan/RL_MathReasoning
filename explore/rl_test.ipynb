{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3938648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from gymnasium import spaces\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578df455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up CUDA device\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "# use a specific GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5,6,7\"\n",
    "\n",
    "# Use GPU for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check the GPU name\n",
    "if device.type == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)  # 0 because CUDA_VISIBLE_DEVICES=4 means GPU 4 is now 0\n",
    "    print(\"Using GPU:\", gpu_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. Supervised Problem Type & Difficulty Classifier\n",
    "# ---------------------------------------------------------------------------\n",
    "def build_label_map(dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        label_map: id -> (type_str, level_int)\n",
    "        encode:  (type_str, level_int) -> id  (for training, if you want it)\n",
    "    \"\"\"\n",
    "    # unique problem categories (7 in the dataset)\n",
    "    types = sorted(set(dataset[\"type\"]))\n",
    "    \n",
    "    # 'Level 3' â†’ 3\n",
    "    def lvl_int(lvl_str): \n",
    "        try:\n",
    "            return int(lvl_str.strip().split()[1])\n",
    "        except (IndexError, ValueError):\n",
    "            return 6 \n",
    "    levels = sorted({lvl_int(lvl) for lvl in dataset[\"level\"]})  # [1-5]\n",
    "    \n",
    "    label_map, encode = {}, {}\n",
    "    for t in types:\n",
    "        for l in levels:\n",
    "            idx = len(label_map)\n",
    "            label_map[idx] = (t, l)\n",
    "            encode[(t, l)] = idx\n",
    "    return label_map, encode\n",
    "\n",
    "class ProblemClassifier:\n",
    "    \"\"\"Lightweight text classifier.  Fineâ€‘tune or plugâ€‘in a checkpoint.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, label_map: Dict[int, Tuple[str, int]]):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.label_map = label_map  # id -> (type, level)\n",
    "        self.model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.to(\"cuda\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, text: str) -> Tuple[str, int]:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        logits = self.model(**inputs).logits[0]\n",
    "        label_id = int(logits.argmax())\n",
    "        return self.label_map[label_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a41662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2. Reasoning Engine (frozen LM) with stepâ€‘wise generation\n",
    "# ---------------------------------------------------------------------------\n",
    "from prompt_template import PROMPT_PREFIX\n",
    "\n",
    "class ReasoningEngine:\n",
    "    \"\"\"Generates stepâ€‘byâ€‘step chainâ€‘ofâ€‘thought until told to stop.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dir: str, max_tokens_per_step: int = 32):\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir, padding_side=\"left\")\n",
    "        self.model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.eval().requires_grad_(False)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.to(\"cuda\")\n",
    "        self.max_tokens_per_step = max_tokens_per_step\n",
    "        self.prefix = PROMPT_PREFIX\n",
    "\n",
    "    def reset_prompt(self, problem_text):\n",
    "        # Called at env.reset()\n",
    "        self.prompt = (\n",
    "            f\"{self.prefix}\\n\"\n",
    "            f\"Problem: {problem_text}<|eot_id|>\"\n",
    "            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        )\n",
    "        return self.prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def think_step(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a *single* reasoning chunk.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        out = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=self.max_tokens_per_step,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        generated = self.tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n",
    "        return generated.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_answer(text: str) -> str:\n",
    "        from math_utils import last_boxed_only_string, normalize_final_answer, remove_boxed\n",
    "\n",
    "        last_boxed_text = last_boxed_only_string(text)\n",
    "        answer = normalize_final_answer(remove_boxed(last_boxed_text)) if last_boxed_text else None\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccf666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathEnv(gym.Env):\n",
    "    \"\"\"Gymâ€‘compatible environment where the agent decides to *think* or *stop*.\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_split: str = \"train[:2048]\",  # small default slice\n",
    "        model_dir: str = \"../../../llm/llama/Llama-3.2-1B-Instruct\",\n",
    "        classifier_name: str = \"distilbert-base-uncased\",\n",
    "        penalty_lambda: float = 0.003,\n",
    "        max_steps: int = 50,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Dataset ------------------------------------------------------------\n",
    "        self.data = load_local_math_dataset(dataset_split, root_dir=\"MATH\")\n",
    "\n",
    "        # Build label map\n",
    "        self.label_map, self.encode = build_label_map(self.data)\n",
    "\n",
    "        # Components ---------------------------------------------------------\n",
    "        self.classifier = ProblemClassifier(classifier_name, self.label_map)\n",
    "        self.engine = ReasoningEngine(model_dir)\n",
    "\n",
    "        # Hyperâ€‘parameters ---------------------------------------------------\n",
    "        self.penalty_lambda = penalty_lambda\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Spaces -------------------------------------------------------------\n",
    "        # Observation: flattened categorical ids â€“ replace w/ embeddings if needed\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)\n",
    "        # Action: 0 = CONTINUE, 1 = STOP\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Internal state -----------------------------------------------------\n",
    "        self.idx = -1\n",
    "        self.problem: Dict = {}\n",
    "        self.prompt = \"\"\n",
    "        self.reasoning_trace: List[str] = []\n",
    "        self.step_count = 0\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Gym API\n",
    "    # ---------------------------------------------------------------------\n",
    "    def reset(self, *, seed: int | None = None, options: Dict | None = None):\n",
    "        if seed is not None:\n",
    "            super().reset(seed=seed)\n",
    "        self.idx = (self.idx + 1) % len(self.data)\n",
    "        self.problem = self.data[self.idx]\n",
    "        self.prompt = self.engine.reset_prompt(self.problem[\"problem\"])  # chainâ€‘ofâ€‘thought prefix\n",
    "        self.reasoning_trace = []\n",
    "        self.step_count = 0\n",
    "\n",
    "\n",
    "        obs = self._build_obs()\n",
    "        info = {\"answer\": self.problem[\"solution\"]}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action: int):\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        info = {}\n",
    "\n",
    "        if action == 0:  # CONTINUE\n",
    "            step_text = self.engine.think_step(self.prompt)\n",
    "            self.reasoning_trace.append(step_text)\n",
    "            self.prompt += step_text + \"\\n\"\n",
    "            self.step_count += 1\n",
    "            done = self.step_count >= self.max_steps  # forced stop\n",
    "            # living penalty per think step\n",
    "            reward -= self.penalty_lambda\n",
    "        else:  # STOP & answer\n",
    "            from math_utils import is_equiv\n",
    "            done = True\n",
    "            generated_answer = self.engine.extract_answer(self.prompt)\n",
    "            gt_answer = self.engine.extract_answer(self.problem[\"solution\"]) \n",
    "\n",
    "            if generated_answer is not None or generated_answer != 24:\n",
    "                try:\n",
    "                    exact_match = int(generated_answer.strip() == gt_answer.strip())\n",
    "                    # or is_equiv(generated_answer, gt_answer)\n",
    "                except AttributeError:\n",
    "                    exact_match = int(generated_answer.strip() == gt_answer)\n",
    "            elif generated_answer is None or generated_answer == 24:\n",
    "                exact_match = 0\n",
    "                reward += -2\n",
    "            else:\n",
    "                exact_match = 0\n",
    "            \n",
    "            correct = exact_match > 0\n",
    "            if correct:\n",
    "                reward += 1.0\n",
    "            else:\n",
    "                reward -= 1.0\n",
    "            info[\"correct\"] = correct\n",
    "            info[\"generated_answer\"] = generated_answer\n",
    "\n",
    "            print(f\"\\nðŸ§  Final prompt:\\n{self.prompt}\")\n",
    "            print(f\"âœ… GT: {gt_answer}\")\n",
    "            print(f\"ðŸ“ Model Response: {generated_answer}\")\n",
    "            print(f\"ðŸŽ¯ Correct: {correct}, Reward: {reward}\") \n",
    "\n",
    "\n",
    "        obs = self._build_obs() if not done else np.zeros_like(self.observation_space.sample())\n",
    "            \n",
    "        return obs, reward, done, False, info  # Gymnasium API (v0.29)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _build_obs(self):\n",
    "        \"\"\"Cheap handâ€‘crafted obs: [type_id, level, step_count, ...padding]\"\"\"\n",
    "        p_type, level = self.classifier.predict(self.problem[\"problem\"])\n",
    "        type_id = hash(p_type) % 7  # up to 7 base types â€“ update as needed\n",
    "        vec = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        vec[0] = type_id / 7.0\n",
    "        vec[1] = level / 5.0  # assume 1â€‘5 scale\n",
    "        vec[2] = self.step_count / float(self.max_steps)\n",
    "        return vec\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def render(self):\n",
    "        print(\"\\nProblem:\", self.problem[\"problem\"])\n",
    "        print(\"\\nReasoning Trace:\")\n",
    "        for t in self.reasoning_trace:\n",
    "            print(\"  >\", t)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
